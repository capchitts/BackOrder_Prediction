{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25a6d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re as re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as mno\n",
    "from numpy import load\n",
    "\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.feature_selection import SelectKBest , f_classif \n",
    "from sklearn.linear_model import LinearRegression , LogisticRegression, SGDClassifier, Ridge\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import lightgbm as lgb\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold ,cross_val_score\n",
    "from sklearn.model_selection import  train_test_split , RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import f1_score , make_scorer\n",
    "from scipy.stats import loguniform\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65db603",
   "metadata": {},
   "source": [
    "# Back Order Prediction\n",
    "    A common challenge in Supply Chain Management , where a company wants to optimize the product storage such that minimum space and maximum profit is gained.\n",
    "    If a company can predict shortage of a product prior then it can store it and make profits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b18199",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <h1>Features</h1>\n",
    "    <body>\n",
    "        <br>\n",
    "        <div><b>sku -</b> sku code A stock-keeping unit (SKU) is a scannable bar code.</div><br>\n",
    "        <div><b>national_inv -</b> Current inventory level of component</div><br>\n",
    "        <div><b>lead_time -</b>Transit time</div><br>\n",
    "        <div><b>in_transit_qtry -</b> Quantity in transit</div><br>\n",
    "        <div><b>forecast_x_month -</b> Forecast sales for the net 3, 6, 9 months</div><br>\n",
    "        <div><b>sales_x_month -</b> Sales quantity for the prior 1, 3, 6, 9 months</div><br>\n",
    "        <div><b>min_bank -</b> Minimum recommended amount in stock</div><br>\n",
    "        <div><b>potential_issue -</b> Indictor variable noting potential issue with item</div><br>\n",
    "        <div><b>pieces_past_due -</b> Parts overdue from source</div><br>\n",
    "        <div><b>perf_x_months_avg -</b> Source performance in the last 6 and 12 months</div><br>\n",
    "        <div><b>local_bo_qty -</b> Amount of stock orders overdue</div><br>\n",
    "        <div><b>X17-X21 -</b> General Risk Flags</div><br>\n",
    "        <div><b>went_on_back_order -</b> Product went on backorder (Target Variable)</div><br>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c65e3",
   "metadata": {},
   "source": [
    "<html>\n",
    "<body>\n",
    "<div>\n",
    "\n",
    "It is a Binary Classification problem , aim of this Case study is to predict whether there is gonna be shortage or not.\n",
    "</div>\n",
    "<br>\n",
    "Target Variable is <b>went_on_backorder</b>\n",
    "<ul>    \n",
    "   <li>0 -> no shortage expected</li> \n",
    "   <li>1-> expected shortage</li>\n",
    "</ul>\n",
    "<body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d697a71",
   "metadata": {},
   "source": [
    "##  Modelling\n",
    "\n",
    "<html>\n",
    "    <body>\n",
    "    <br>\n",
    "    <b>Metric</b>\n",
    "    <div>F1-score because  False Positive and False Negative are both useful for making profit.</div><br>\n",
    "\t<b>Modelling</b>\n",
    "     <div>\n",
    "        1.Dummy Model<br>\n",
    "        2.Linear Models(Log Reg , SGDClassifier)<br>\n",
    "        3.Non Linear Models(Decision Tree)<br>\n",
    "        4.Ensembles(RF , XGBoost)<br>\n",
    "    </div>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337219f",
   "metadata": {},
   "source": [
    "###  Dummy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f21be1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd14a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are SMOTE only sampled datasets\n",
    "X_1 = load('X_1.npy')\n",
    "y_1 = load('y_1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a044a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are SMOTE with undersampling datasets\n",
    "X_test = load('X_2.npy')\n",
    "y_test = load('y_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d48cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aa594ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score using SMOTE for oversampling: 0.013526145562639048\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "model_d = DummyClassifier(strategy=\"uniform\")\n",
    "model_d.fit(X_1,y_1)\n",
    "y_pred = model_d.predict(X_test_fs)\n",
    "f1_score_dummy = f1_score(test_y,y_pred)\n",
    "print(\"F1_score using SMOTE for oversampling: {}\".format(f1_score_dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f514fd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score using SMOTE for oversampling: 0.013230884557721138\n"
     ]
    }
   ],
   "source": [
    "#SMOTE with undersampling\n",
    "model_d2 = DummyClassifier(strategy=\"uniform\")\n",
    "model_d2.fit(X_2,y_2)\n",
    "y_pred2 = model_d2.predict(X_test_fs)\n",
    "f1_score_dummy1 = f1_score(test_y,y_pred2)\n",
    "print(\"F1_score using SMOTE for oversampling: {}\".format(f1_score_dummy1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a06fdd",
   "metadata": {},
   "source": [
    "### Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77365fdd",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46dd1638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.643162 using {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.01}\n",
      "0.643012 (0.000723) with: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 10}\n",
      "0.643079 (0.000751) with: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.01}\n",
      "0.643154 (0.000799) with: {'solver': 'liblinear', 'penalty': 'l2', 'C': 10}\n",
      "0.643092 (0.000852) with: {'solver': 'liblinear', 'penalty': 'l2', 'C': 100}\n",
      "0.643162 (0.000856) with: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.01}\n",
      "0.643141 (0.000783) with: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.1}\n",
      "0.566548 (0.061833) with: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 1.0}\n",
      "0.643014 (0.000723) with: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 1.0}\n",
      "0.643023 (0.000698) with: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.1}\n",
      "0.643090 (0.000808) with: {'solver': 'liblinear', 'penalty': 'l2', 'C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "f1_scorer = make_scorer(f1_score)\n",
    "model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "r_cv = RandomizedSearchCV(model,grid, n_jobs=-1, cv=cv, scoring= f1_scorer,error_score=0)\n",
    "grid_result = r_cv.fit(X_1, y_1)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f346361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5979903222563487\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(solver = 'liblinear', penalty = 'l2', C = 0.01)\n",
    "log_reg.fit(X_1,y_1)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "log_reg_f1 = f1_score(y_test,y_pred,average='macro')\n",
    "print(log_reg_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6498b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_f1 = 0.5979903222563487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d103420b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.641720 using {'solver': 'liblinear', 'penalty': 'l2', 'C': 10}\n",
      "0.641720 (0.001042) with: {'solver': 'liblinear', 'penalty': 'l2', 'C': 10}\n",
      "0.545748 (0.057331) with: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.01}\n",
      "0.641718 (0.000899) with: {'solver': 'liblinear', 'penalty': 'l2', 'C': 100}\n",
      "0.559142 (0.060236) with: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 10}\n",
      "0.641651 (0.000866) with: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 1.0}\n",
      "0.541897 (0.052710) with: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 1.0}\n",
      "0.641664 (0.000866) with: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.1}\n",
      "0.641654 (0.000812) with: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.01}\n",
      "0.530849 (0.045216) with: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 100}\n",
      "0.525901 (0.040471) with: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "f1_scorer = make_scorer(f1_score)\n",
    "model1_lr = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "r_cv = RandomizedSearchCV(model1_lr,grid, n_jobs=-1, cv=cv, scoring= f1_scorer,error_score=0)\n",
    "grid_result = r_cv.fit(X_2, y_2)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d2b8a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a sample of this data set to work with SVM classifiers\n",
    "X_1s = X_1[:1000000]\n",
    "y_1s = y_1[:1000000]\n",
    "X_2s = X_2[:1000000]\n",
    "y_2s = y_2[:1000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770b3bb3",
   "metadata": {},
   "source": [
    "### Non Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f3692",
   "metadata": {},
   "source": [
    "#### Decisiton Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99c4391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.778362 using {'min_samples_leaf': 10, 'max_depth': 20, 'criterion': 'gini'}\n",
      "0.537912 (0.001260) with: {'min_samples_leaf': 100, 'max_depth': 3, 'criterion': 'gini'}\n",
      "0.240704 (0.001188) with: {'min_samples_leaf': 20, 'max_depth': 2, 'criterion': 'gini'}\n",
      "0.773723 (0.001051) with: {'min_samples_leaf': 50, 'max_depth': 20, 'criterion': 'gini'}\n",
      "0.778362 (0.001010) with: {'min_samples_leaf': 10, 'max_depth': 20, 'criterion': 'gini'}\n",
      "0.240704 (0.001188) with: {'min_samples_leaf': 20, 'max_depth': 2, 'criterion': 'entropy'}\n",
      "0.240704 (0.001188) with: {'min_samples_leaf': 50, 'max_depth': 2, 'criterion': 'entropy'}\n",
      "0.624105 (0.002672) with: {'min_samples_leaf': 50, 'max_depth': 10, 'criterion': 'entropy'}\n",
      "0.540683 (0.001190) with: {'min_samples_leaf': 5, 'max_depth': 3, 'criterion': 'entropy'}\n",
      "0.538765 (0.001284) with: {'min_samples_leaf': 10, 'max_depth': 5, 'criterion': 'gini'}\n",
      "0.541678 (0.001214) with: {'min_samples_leaf': 5, 'max_depth': 5, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "model_dt1 = DecisionTreeClassifier()\n",
    "\n",
    "params = {\n",
    "    'max_depth': [2, 3, 5, 10, 20],\n",
    "    'min_samples_leaf': [5, 10, 20, 50, 100],\n",
    "    'criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "r_cv = RandomizedSearchCV(model_dt1, params, n_jobs=-1, cv=cv, scoring=f1_scorer,error_score=0)\n",
    "grid_result = r_cv.fit(X_1, y_1)\n",
    "# summarize results`\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0242a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(min_samples_leaf = 10, max_depth = 20, criterion = 'gini')\n",
    "dt.fit(X_1,y_1)\n",
    "y_pred = dt.predict(X_test)\n",
    "dt_f1 = f1_score(y_true=y_test,y_pred=y_pred,average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c707086",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2343bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_f1 = 0.7806553008736472"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87874750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.774104 using {'min_samples_leaf': 20, 'max_depth': 20, 'criterion': 'gini'}\n",
      "0.623425 (0.004512) with: {'min_samples_leaf': 20, 'max_depth': 10, 'criterion': 'entropy'}\n",
      "0.770626 (0.001474) with: {'min_samples_leaf': 50, 'max_depth': 20, 'criterion': 'gini'}\n",
      "0.538490 (0.001617) with: {'min_samples_leaf': 5, 'max_depth': 3, 'criterion': 'entropy'}\n",
      "0.547005 (0.003295) with: {'min_samples_leaf': 10, 'max_depth': 5, 'criterion': 'gini'}\n",
      "0.765163 (0.001341) with: {'min_samples_leaf': 100, 'max_depth': 20, 'criterion': 'gini'}\n",
      "0.765340 (0.001139) with: {'min_samples_leaf': 100, 'max_depth': 20, 'criterion': 'entropy'}\n",
      "0.623178 (0.004501) with: {'min_samples_leaf': 50, 'max_depth': 10, 'criterion': 'entropy'}\n",
      "0.538196 (0.001598) with: {'min_samples_leaf': 100, 'max_depth': 3, 'criterion': 'gini'}\n",
      "0.772233 (0.001196) with: {'min_samples_leaf': 20, 'max_depth': 20, 'criterion': 'entropy'}\n",
      "0.774104 (0.001490) with: {'min_samples_leaf': 20, 'max_depth': 20, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "model_dt2 = DecisionTreeClassifier()\n",
    "\n",
    "params = {\n",
    "    'max_depth': [2, 3, 5, 10, 20],\n",
    "    'min_samples_leaf': [5, 10, 20, 50, 100],\n",
    "    'criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "r_cv = RandomizedSearchCV(model_dt2, params, n_jobs=-1, cv=cv, scoring=f1_scorer,error_score=0)\n",
    "grid_result = r_cv.fit(X_2, y_2)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba91b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "836f81ef",
   "metadata": {},
   "source": [
    "### Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282b979b",
   "metadata": {},
   "source": [
    "#### Bagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "994fece5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.158604 using {'n_estimators': 100}\n",
      "0.147559 (0.017059) with: {'n_estimators': 10}\n",
      "0.158604 (0.016763) with: {'n_estimators': 100}\n",
      "0.158181 (0.016835) with: {'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "# define models and parameters\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "bagging1 = BaggingClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "# define grid search\n",
    "grid = dict(n_estimators=n_estimators)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "r_cv = RandomizedSearchCV(bagging1, grid, n_jobs=-1, cv=cv, scoring=f1_scorer,error_score=0)\n",
    "grid_result = r_cv.fit(X_1s, y_1s)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f921b154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.000000 using {'n_estimators': 10}\n",
      "0.000000 (0.000000) with: {'n_estimators': 10}\n",
      "0.000000 (0.000000) with: {'n_estimators': 100}\n",
      "0.000000 (0.000000) with: {'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "# define models and parameters\n",
    "bagging2 = BaggingClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "# define grid search\n",
    "grid = dict(n_estimators=n_estimators)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "r_cv = RandomizedSearchCV(bagging2, grid, n_jobs=-1, cv=cv, scoring=f1_scorer,error_score=0)\n",
    "grid_result = r_cv.fit(X_2s, y_2s)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736fb1e4",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "917da6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.155176 using {'n_estimators': 100, 'max_features': 'log2'}\n",
      "0.143770 (0.016685) with: {'n_estimators': 10, 'max_features': 'sqrt'}\n",
      "0.154178 (0.016223) with: {'n_estimators': 100, 'max_features': 'sqrt'}\n",
      "0.153327 (0.017025) with: {'n_estimators': 1000, 'max_features': 'sqrt'}\n",
      "0.146845 (0.018499) with: {'n_estimators': 10, 'max_features': 'log2'}\n",
      "0.155176 (0.015976) with: {'n_estimators': 100, 'max_features': 'log2'}\n",
      "0.153523 (0.016242) with: {'n_estimators': 1000, 'max_features': 'log2'}\n"
     ]
    }
   ],
   "source": [
    "rf1 = RandomForestClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "max_features = ['sqrt', 'log2']\n",
    "# define grid search\n",
    "grid = dict(n_estimators=n_estimators,max_features=max_features)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "r_cv = RandomizedSearchCV(rf1, grid, n_jobs=-1, cv=cv, scoring=f1_scorer,error_score=0)\n",
    "grid_result = r_cv.fit(X_1s, y_1s)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf14cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b8753a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.000000 using {'n_estimators': 10, 'max_features': 'sqrt'}\n",
      "0.000000 (0.000000) with: {'n_estimators': 10, 'max_features': 'sqrt'}\n",
      "0.000000 (0.000000) with: {'n_estimators': 100, 'max_features': 'sqrt'}\n",
      "0.000000 (0.000000) with: {'n_estimators': 1000, 'max_features': 'sqrt'}\n",
      "0.000000 (0.000000) with: {'n_estimators': 10, 'max_features': 'log2'}\n",
      "0.000000 (0.000000) with: {'n_estimators': 100, 'max_features': 'log2'}\n",
      "0.000000 (0.000000) with: {'n_estimators': 1000, 'max_features': 'log2'}\n"
     ]
    }
   ],
   "source": [
    "rf2 = RandomForestClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "max_features = ['sqrt', 'log2']\n",
    "# define grid search\n",
    "grid = dict(n_estimators=n_estimators,max_features=max_features)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "r_cv = RandomizedSearchCV(rf2, grid, n_jobs=-1, cv=cv, scoring=f1_scorer,error_score=0)\n",
    "grid_result = r_cv.fit(X_2s, y_2s)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de06030a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1637d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28362d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3391d14a",
   "metadata": {},
   "source": [
    "#### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed61a2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.130724 using {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 9, 'learning_rate': 0.1}\n",
      "0.000000 (0.000000) with: {'subsample': 1.0, 'n_estimators': 10, 'max_depth': 9, 'learning_rate': 0.01}\n",
      "0.001272 (0.001805) with: {'subsample': 0.5, 'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.001}\n",
      "0.130724 (0.017961) with: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 9, 'learning_rate': 0.1}\n",
      "0.014637 (0.008335) with: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 9, 'learning_rate': 0.01}\n",
      "0.000870 (0.002383) with: {'subsample': 0.5, 'n_estimators': 10, 'max_depth': 3, 'learning_rate': 0.1}\n",
      "0.000000 (0.000000) with: {'subsample': 1.0, 'n_estimators': 100, 'max_depth': 9, 'learning_rate': 0.001}\n",
      "0.000000 (0.000000) with: {'subsample': 0.5, 'n_estimators': 10, 'max_depth': 7, 'learning_rate': 0.001}\n",
      "0.025677 (0.010943) with: {'subsample': 1.0, 'n_estimators': 1000, 'max_depth': 9, 'learning_rate': 0.001}\n",
      "0.064510 (0.016277) with: {'subsample': 0.5, 'n_estimators': 10, 'max_depth': 7, 'learning_rate': 0.1}\n",
      "0.000000 (0.000000) with: {'subsample': 0.7, 'n_estimators': 10, 'max_depth': 7, 'learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# define models and parameters\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "xgboost1 = GradientBoostingClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "subsample = [0.5, 0.7, 1.0]\n",
    "max_depth = [3, 7, 9]\n",
    "# define grid search\n",
    "grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "r_cv = RandomizedSearchCV(xgboost1, grid, n_jobs=-1, cv=cv, scoring=f1_scorer,error_score=0)\n",
    "grid_result = r_cv.fit(X_1s, y_1s)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a485208f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d71342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de2607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f36a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007026bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models and parameters\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "xgboost1 = GradientBoostingClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "subsample = [0.5, 0.7, 1.0]\n",
    "max_depth = [3, 7, 9]\n",
    "# define grid search\n",
    "grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "r_cv = RandomizedSearchCV(xgboost1, grid, n_jobs=-1, cv=cv, scoring=f1_scorer,error_score=0)\n",
    "grid_result = r_cv.fit(X_1, y_1)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e43f753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost1 = GradientBoostingClassifier(subsample= 0.7, n_estimators= 1000, max_depth= 9, learning_rate=0.1)\n",
    "xgboost1.fit(X_1,y_1)\n",
    "y_pred = xgboost1.predict(X_test)\n",
    "xgb_f1_score = f1_score(y_true=y_test,y_pred=y_pred,average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c42f070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82302628606276\n"
     ]
    }
   ],
   "source": [
    "print(xgb_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10abf1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_f1_score =0.8231992657663574"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00f03ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_model.sav']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "filename = 'final_model.sav'\n",
    "joblib.dump(xgboost1, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c8f04b",
   "metadata": {},
   "source": [
    "####Order of features after feature selection####<br>\n",
    "0         lead_time     \n",
    "1    in_transit_qty       \n",
    "2          min_bank       \n",
    "3   potential_issue     \n",
    "4  perf_6_month_avg     \n",
    "5      local_bo_qty     \n",
    "6         deck_risk     \n",
    "7     oe_constraint      \n",
    "8         ppap_risk      \n",
    "9     stop_auto_buy       \n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "498150f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.   0.   0.   0.   0.95 0.   1.   0.   0.   1.  ]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbe763a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(xgboost1.predict(X_test[123].reshape(1,-1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceacdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models and parameters\n",
    "f1_scorer = make_scorer(f1_score,average='macro')\n",
    "\n",
    "xgboost1 = GradientBoostingClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "subsample = [0.5, 0.7, 1.0]\n",
    "max_depth = [3, 7, 9]\n",
    "# define grid search\n",
    "grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "r_cv = RandomizedSearchCV(xgboost1, grid, n_jobs=-1, cv=cv, scoring=f1_scorer,error_score=0)\n",
    "grid_result = r_cv.fit(X_1, y_1)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost1 = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44302b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d002347d",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01eb0c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d36153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.752849 using {'subsample': 0.5, 'random_state': 501, 'num_leaves': 200, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.746519 (0.002141) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 200, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.713210 (0.000656) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 90, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.683763 (0.001092) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 200, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.752849 (0.002293) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 200, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.718409 (0.000977) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 90, 'max_depth': 8, 'learning_rate': 0.01, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.690001 (0.001225) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 200, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.693802 (0.001372) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 200, 'max_depth': 7, 'learning_rate': 0.001, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.684271 (0.001044) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 90, 'max_depth': 5, 'learning_rate': 0.001, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.718409 (0.000977) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 90, 'max_depth': 8, 'learning_rate': 0.01, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.672108 (0.001313) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 200, 'max_depth': 5, 'learning_rate': 0.001, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n"
     ]
    }
   ],
   "source": [
    "f1_scorer = make_scorer(f1_score,average='macro')\n",
    "\n",
    "grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'num_leaves': [90,200],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'max_depth' : [5,6,7,8],\n",
    "    'random_state' : [501], \n",
    "    'colsample_bytree' : [0.5,0.7],\n",
    "    'subsample' : [0.5,0.7]\n",
    "    }\n",
    "\n",
    "lgb_1 = lgb.LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "r_cv = RandomizedSearchCV(lgb_1, grid, n_jobs=-1, cv=cv, scoring=f1_scorer,error_score=0)\n",
    "grid_result = r_cv.fit(X_1, y_1)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "702c55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = lgb.LGBMClassifier(subsample= 0.5, random_state = 501, num_leaves= 200, \n",
    "                   max_depth = 7, learning_rate = 0.1, colsample_bytree = 0.5, boosting_type = 'gbdt')\n",
    "lgb_clf.fit(X_1,y_1)\n",
    "y_pred = lgb_clf.predict(X_test)\n",
    "lgb_f1 = f1_score(y_true = y_test,y_pred = y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af080ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7539856580054936\n"
     ]
    }
   ],
   "source": [
    "print(lgb_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b16faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_f1 = 0.7539856580054936"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ea85e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.747126 using {'subsample': 0.7, 'random_state': 501, 'num_leaves': 90, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.747126 (0.002102) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 90, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.668578 (0.001246) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 90, 'max_depth': 5, 'learning_rate': 0.001, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.700692 (0.001301) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 90, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.676374 (0.003244) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 90, 'max_depth': 7, 'learning_rate': 0.001, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.667538 (0.001621) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 90, 'max_depth': 6, 'learning_rate': 0.001, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.700843 (0.001252) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 90, 'max_depth': 6, 'learning_rate': 0.01, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.700843 (0.001252) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 90, 'max_depth': 6, 'learning_rate': 0.01, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.700843 (0.001252) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 200, 'max_depth': 6, 'learning_rate': 0.01, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.730686 (0.002523) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 90, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.713043 (0.001729) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 200, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n"
     ]
    }
   ],
   "source": [
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'num_leaves': [90,200],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'max_depth' : [5,6,7,8],\n",
    "    'random_state' : [501], \n",
    "    'colsample_bytree' : [0.5,0.7],\n",
    "    'subsample' : [0.5,0.7]\n",
    "    }\n",
    "\n",
    "lgb_1 = lgb.LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "r_cv = RandomizedSearchCV(lgb_1, grid, n_jobs=-1, cv=cv, scoring=f1_scorer,error_score=0)\n",
    "grid_result = r_cv.fit(X_1, y_1)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29c38839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.763942 using {'subsample': 0.7, 'random_state': 501, 'num_leaves': 90, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.649445 (0.001883) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 200, 'max_depth': 5, 'learning_rate': 0.001, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.675832 (0.002196) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 200, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.720564 (0.001046) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 90, 'max_depth': 8, 'learning_rate': 0.01, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.713509 (0.001623) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 200, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.700652 (0.001491) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 90, 'max_depth': 6, 'learning_rate': 0.01, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.754985 (0.001949) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 200, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.706778 (0.002736) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 200, 'max_depth': 8, 'learning_rate': 0.001, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.700966 (0.001246) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 90, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.708632 (0.001075) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 90, 'max_depth': 7, 'learning_rate': 0.001, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.763942 (0.002192) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 90, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n"
     ]
    }
   ],
   "source": [
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'num_leaves': [90,200],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'max_depth' : [5,6,7,8],\n",
    "    'random_state' : [501], \n",
    "    'colsample_bytree' : [0.5,0.7],\n",
    "    'subsample' : [0.5,0.7]\n",
    "    }\n",
    "\n",
    "lgb_2 = lgb.LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "r_cv = RandomizedSearchCV(lgb_2, grid, n_jobs=-1, cv=cv, scoring=f1_scorer,error_score=0)\n",
    "grid_result = r_cv.fit(X_2, y_2)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70ae9a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.771447 using {'subsample': 0.7, 'random_state': 501, 'num_leaves': 200, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.704746 (0.000969) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 200, 'max_depth': 7, 'learning_rate': 0.001, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.745352 (0.001950) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 200, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.753403 (0.001615) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 90, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.771447 (0.001831) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 200, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.759066 (0.001580) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 200, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.759066 (0.001580) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 200, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.713112 (0.000949) with: {'subsample': 0.7, 'random_state': 501, 'num_leaves': 200, 'max_depth': 8, 'learning_rate': 0.001, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.706188 (0.001105) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 200, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n",
      "0.743271 (0.002373) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 90, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
      "0.683466 (0.001294) with: {'subsample': 0.5, 'random_state': 501, 'num_leaves': 90, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7, 'boosting_type': 'gbdt'}\n"
     ]
    }
   ],
   "source": [
    "f1_scorer = make_scorer(f1_score,average='macro')\n",
    "\n",
    "grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'num_leaves': [90,200],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'max_depth' : [5,6,7,8],\n",
    "    'random_state' : [501], \n",
    "    'colsample_bytree' : [0.5,0.7],\n",
    "    'subsample' : [0.5,0.7]\n",
    "    }\n",
    "\n",
    "lgb_2 = lgb.LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "r_cv = RandomizedSearchCV(lgb_2, grid, n_jobs=-1, cv=cv, scoring=f1_scorer,error_score=0)\n",
    "grid_result = r_cv.fit(X_2, y_2)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f139b5",
   "metadata": {},
   "source": [
    "#### Making a stacking classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33abd284",
   "metadata": {},
   "source": [
    "\n",
    "<ol>\n",
    "<li>Using dataset A i.e SMOTE without under sampling.</li>\n",
    "<li><ul>Base Learners used are following with pre trained hyperparameters:\n",
    "                <li>Logistic Regression</li>\n",
    "                <li>GradientBoostingClassifier</li>\n",
    "                <li>DecisionTreeClassifier</li>\n",
    "                <li>LGBM</li></ul></li>\n",
    " <li>Using Logistic Regression as meta learner.</li></ol></html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78569c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are SMOTE only sampled datasets\n",
    "X_train = load('X_1.npy')\n",
    "y_train = load('y_1.npy')\n",
    "X_test  = load('X_1_test.npy')\n",
    "y_test = load('y_1_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c2e4c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2514850, 10)\n",
      "(421965, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e700e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(solver='liblinear', penalty='l2', C = 0.01)\n",
    "clf2 = DecisionTreeClassifier(min_samples_leaf= 10, max_depth= 20, criterion ='gini')\n",
    "clf3 = GradientBoostingClassifier(subsample = 0.7, n_estimators = 1000, max_depth = 9, learning_rate = 0.1)\n",
    "level_1 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb183c5",
   "metadata": {},
   "source": [
    "####  Stacking_Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87e619c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4733751633061474"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_learners = [\n",
    "                 ('log_reg', LogisticRegression(solver='liblinear', penalty='l2', C = 0.01)),\n",
    "                 ('dt', DecisionTreeClassifier(min_samples_leaf= 10, max_depth= 20, criterion ='gini')) ,\n",
    "                 ('gbdt',GradientBoostingClassifier(subsample = 0.7, n_estimators = 1000, max_depth = 9, learning_rate = 0.1))\n",
    "                ]\n",
    "\n",
    "# Initialize Stacking Classifier with the Meta Learner\n",
    "clf = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n",
    "\n",
    "# Extract score\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "f1_score(y_true=y_test,y_pred=y_pred,average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397534ae",
   "metadata": {},
   "source": [
    "#### Stacking_Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d683cb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4736268603406309"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_learners = [\n",
    "                 ('log_reg', LogisticRegression(solver='liblinear', penalty='l2', C = 0.01)),\n",
    "                 ('dt', DecisionTreeClassifier(min_samples_leaf= 10, max_depth= 20, criterion ='gini')) ,\n",
    "                 ('gbdt',GradientBoostingClassifier(subsample = 0.7, n_estimators = 1000, max_depth = 9, learning_rate = 0.1))\n",
    "                ]\n",
    "\n",
    "# Initialize Stacking Classifier with the Meta Learner\n",
    "clf = StackingClassifier(estimators=base_learners,\n",
    "                         final_estimator=LogisticRegression(solver='liblinear', penalty='l2', C = 0.01))\n",
    "\n",
    "# Extract score\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "f1_score(y_true=y_test,y_pred=y_pred,average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29bdd78",
   "metadata": {},
   "source": [
    "#### Stacking_Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0bf7644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4693022759166582"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_learners = [\n",
    "                 ('log_reg', LogisticRegression(solver='liblinear', penalty='l2', C = 0.01)),\n",
    "                 ('dt', DecisionTreeClassifier(min_samples_leaf= 10, max_depth= 20, criterion ='gini')) ,\n",
    "                 ('gbdt',GradientBoostingClassifier(subsample = 0.7, n_estimators = 1000, max_depth = 9, learning_rate = 0.1)),\n",
    "                 ('lgb',lgb.LGBMClassifier(subsample= 0.5, random_state = 501, num_leaves= 200, \n",
    "                   max_depth = 7, learning_rate = 0.1, colsample_bytree = 0.5, boosting_type = 'gbdt'))\n",
    "                ]\n",
    "\n",
    "# Initialize Stacking Classifier with the Meta Learner\n",
    "clf = StackingClassifier(estimators=base_learners,\n",
    "                         final_estimator=GradientBoostingClassifier(subsample = 0.7, \n",
    "                                                                    n_estimators = 1000,\n",
    "                                                                    max_depth = 9, \n",
    "                                                                    learning_rate = 0.1))\n",
    "\n",
    "# Extract score\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "f1_score(y_true=y_test,y_pred=y_pred,average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af702a53",
   "metadata": {},
   "source": [
    "#### Stacking_Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e024136",
   "metadata": {},
   "source": [
    "##### Define the Level 0 classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fd88e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(solver='liblinear', penalty='l2', C = 0.01)\n",
    "clf2 = DecisionTreeClassifier(min_samples_leaf= 10, max_depth= 20, criterion ='gini')\n",
    "clf3 = GradientBoostingClassifier(subsample = 0.7, n_estimators = 1000, max_depth = 9, learning_rate = 0.1)\n",
    "clf4 = lgb.LGBMClassifier(subsample= 0.5, random_state = 501, num_leaves= 200, \n",
    "                   max_depth = 7, learning_rate = 0.1, colsample_bytree = 0.5, boosting_type = 'gbdt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfb3931",
   "metadata": {},
   "source": [
    "#### Define the Level 1 classifiers or Meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39ced9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sclf = StackingCVClassifier(classifiers = [clf1, clf2, clf3, clf4],\n",
    "                            cv = 5,\n",
    "                            meta_classifier = LogisticRegression(solver='liblinear', penalty='l2', C = 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43682595",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\"Log_Reg\": clf1,\n",
    "               \"CART\": clf2,\n",
    "               \"GBDT\": clf3,\n",
    "               \"LGBM\": clf4,\n",
    "               \"Stacking_Classifier\": sclf}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeea3e12",
   "metadata": {},
   "source": [
    "#### Train the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28e0480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in classifiers:\n",
    "    # Get classifier\n",
    "    classifier = classifiers[key]\n",
    "    \n",
    "    # Fit classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "        \n",
    "    # Save fitted classifier\n",
    "    classifiers[key] = classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74204d",
   "metadata": {},
   "source": [
    "#### Make a dataframe for storing predicted class and f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55edf1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log_Reg->0.337264996214767\n",
      "CART->0.4503255702360726\n",
      "GBDT->0.467952782655212\n",
      "LGBM->0.4432023048619828\n",
      "Stacking_Classifier->0.467923417937662\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Classifier','F1_score'])\n",
    "for key in classifiers:\n",
    "    # Make prediction on test set\n",
    "    y_pred = classifiers[key].predict(X_test)\n",
    "    # Save results in pandas dataframe object\n",
    "    results.loc[len(results.index)] = [key,f1_score(y_test,y_pred,average='macro')]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bba28ff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Log_Reg</td>\n",
       "      <td>0.337265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CART</td>\n",
       "      <td>0.450326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GBDT</td>\n",
       "      <td>0.467953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.443202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stacking_Classifier</td>\n",
       "      <td>0.467923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  F1_score\n",
       "0              Log_Reg  0.337265\n",
       "1                 CART  0.450326\n",
       "2                 GBDT  0.467953\n",
       "3                 LGBM  0.443202\n",
       "4  Stacking_Classifier  0.467923"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13033872",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e4066d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------------------+--------------------------------------------+\n",
      "|       Classifier Name        |   F1_Score-macro   |               Sampling Used                |\n",
      "+------------------------------+--------------------+--------------------------------------------+\n",
      "|      Logistic Reression      | 0.5979903222563487 |                   SMOTE                    |\n",
      "|        Decision Tree         | 0.7806553008736472 |                   SMOTE                    |\n",
      "|           XGBoost            | 0.8231992657663574 |                   SMOTE                    |\n",
      "|             LGBM             | 0.7539856580054936 |                   SMOTE                    |\n",
      "|       Stacking Model 1       | 0.4733751633061474 |                   SMOTE                    |\n",
      "|       Stacking Model 2       | 0.4736268603406309 |                   SMOTE                    |\n",
      "|       Stacking Model 3       | 0.4693022759166582 |                   SMOTE                    |\n",
      "|       Stacking Model 4       | 0.467923417937662  |                   SMOTE                    |\n",
      "| Kaggle Public Score(Top 20%) |      0.93301       | XGBoost with all features and sku as index |\n",
      "| Kaggle Private Score(Top 7%) |      0.93901       | XGBoost with all features and sku as index |\n",
      "+------------------------------+--------------------+--------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "  \n",
    "# Specify the Column Names while initializing the Table\n",
    "myTable = PrettyTable([\"Classifier Name\", \"F1_Score-macro\",\"Sampling Used\"])\n",
    "  \n",
    "# Add rows\n",
    "myTable.add_row([\"Logistic Reression\",log_reg_f1 ,\"SMOTE\"])\n",
    "myTable.add_row([\"Decision Tree\", dt_f1,\"SMOTE\"])\n",
    "myTable.add_row([\"XGBoost\", xgb_f1_score,\"SMOTE\"])\n",
    "myTable.add_row([\"LGBM\", lgb_f1,\"SMOTE\"])\n",
    "myTable.add_row([\"Stacking Model 1\", 0.4733751633061474,\"SMOTE\"])\n",
    "myTable.add_row([\"Stacking Model 2\", 0.4736268603406309,\"SMOTE\"])\n",
    "myTable.add_row([\"Stacking Model 3\", 0.4693022759166582,\"SMOTE\"])\n",
    "myTable.add_row([\"Stacking Model 4\", 0.467923417937662,\"SMOTE\"])\n",
    "myTable.add_row([\"Kaggle Public Score(Top 20%)\", 0.93301,\"XGBoost with all features and sku as index\"])\n",
    "myTable.add_row([\"Kaggle Private Score(Top 7%)\",0.93901,\"XGBoost with all features and sku as index\"])\n",
    "\n",
    "\n",
    "print(myTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe52e25c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
